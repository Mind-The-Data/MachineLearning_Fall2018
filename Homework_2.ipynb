{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 (Due Sep. 27) \n",
    "##### Brian Stampe        790 586 017\n",
    "Problems marked with a (\\*) are only required for graduate students.  Undergrads may undertake them for extra credit worth half the problems point value, with no penalties incurred for an incorrect answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bayesian Networks/Conditional Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider three binary variables $a, b, c \\in \\{0, 1\\}$ having the joint distribution given by  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.192 0.144]\n",
      "  [0.048 0.216]]\n",
      "\n",
      " [[0.192 0.064]\n",
      "  [0.048 0.096]]]\n",
      " \n",
      "[[0.384 0.208]\n",
      " [0.096 0.312]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#        | a | b | c | P(a,b,c)\n",
    "P_table = (\n",
    "np.array([[0,  0,  0,  0.192 ],\n",
    "          [0,  0,  1,  0.144 ],\n",
    "          [0,  1,  0,  0.048 ],\n",
    "          [0,  1,  1,  0.216 ],\n",
    "          [1,  0,  0,  0.192 ],\n",
    "          [1,  0,  1,  0.064 ],\n",
    "          [1,  1,  0,  0.048 ],\n",
    "          [1,  1,  1,  0.096 ]]))\n",
    "\n",
    "# Convert P_table (which is a probability table) to a probability array, which \n",
    "# is much more computationally sensible for a discrete valued distribution\n",
    "add = 0\n",
    "P_abc = np.zeros((2,2,2))\n",
    "for entry in P_table:\n",
    "    a = int(entry[0])\n",
    "    b = int(entry[1])\n",
    "    c = int(entry[2])\n",
    "    P = entry[3]\n",
    "    P_abc[a,b,c] = P\n",
    "    #print (P)\n",
    "    #add += P\n",
    "    #print (add)\n",
    "    \n",
    "#print (P_table)\n",
    "\n",
    "print (P_abc)\n",
    "print (' ')\n",
    "print (np.sum(P_abc,axis=0, keepdims=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you may want to use the following functions to help you in the following problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginalize(P,variables_to_marginalize=(),keepdims=True):\n",
    "    \"\"\" Marginalize a probability table, i.e. compute P(a,b) = sum_c P(a,b,c) \n",
    "    \n",
    "        Arguments: \n",
    "        \n",
    "        P (array) -> a probability array in which the dimensions correspond to random variables,\n",
    "                     the indices to variable values, and the entries to probabilities\n",
    "        variables_to_marginalize (tuple) -> a list of integers containing the \n",
    "                                    variable numbers to marginalize over\n",
    "        keepdims (boolean) -> Marginalization reduces the dimensionality of the distribution.  keepdims=False\n",
    "                       removes that dimension from the array indexing scheme.  For example P(a,b,c) has\n",
    "                       an array that is 2x2x2.  If we marginalize over variable/index 1 (aka b), then the \n",
    "                       resulting array P(a,c) is 2x2 if keepdims=False (which may change the index of a \n",
    "                       given variable.  For example, after marginalizing, variable/index 1 is now c).  \n",
    "                       Conversely, the array becomes 2x1x2 if keepdims=True.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Sum over the axes given in variables_to_marginalize\n",
    "    return np.sum(P,axis=variables_to_marginalize,keepdims=keepdims)\n",
    "\n",
    "def condition(P,variables_to_condition=()):\n",
    "    \"\"\" Condition a probability table, i.e. compute P(a|c) = P(a,c)/P(c) \n",
    "    \n",
    "        Arguments: \n",
    "        \n",
    "        P (array) -> a probability array in which the dimensions correspond to random variables,\n",
    "                     the indices to variable values, and the entries to probabilities\n",
    "        variables_to_condition (tuple) -> a list of integers containing the variables to condition on\n",
    "        \n",
    "        NOTE: This function always returns an array that is the same size as the input.  However, this array\n",
    "        no longer sums to one as does an unconditioned joint distribution.  Instead, it sums to one only given\n",
    "        a particular value of the conditioned variable/index.  For example:\n",
    "        \n",
    "        P_abc.sum()==1 -> True\n",
    "        P_ab_given_c = condition(P_abc,variables_to_condition=(2,))\n",
    "        P_ab_given_c.sum()==1 -> False\n",
    "        P_ab_given_c[:,:,0].sum()==1 -> True\n",
    "        P_ab_given_c[:,:,1].sum()==1 -> True\n",
    "   \n",
    "    \"\"\"\n",
    "    # find the variables to marginalize over to get the marginal distribuion of the\n",
    "    # variables that we wish to condition on\n",
    "    v_to_m = list(range(P.ndim))\n",
    "    for v in variables_to_condition:\n",
    "        v_to_m.remove(v)\n",
    "    # Compute the conditional by dividing the input by the output\n",
    "    return P/marginalize(P,variables_to_marginalize=tuple(v_to_m),keepdims=False)\n",
    "    \n",
    "P_a = marginalize(P_abc, variables_to_marginalize=(1,2))\n",
    "P_ac = marginalize(P_abc, variables_to_marginalize=(1,))\n",
    "P_bc = marginalize(P_abc, variables_to_marginalize=(0,))\n",
    "\n",
    "#keepdims=True\n",
    "P_c_given_a  = condition(P_ac, variables_to_condition=(0,))\n",
    "P_b_given_c  = condition(P_bc, variables_to_condition=(2,))\n",
    "\n",
    "#keepdims=False\n",
    "#P_c_given_a  = condition(P_ac, variables_to_condition=(0,))\n",
    "#P_b_given_c  = condition(P_bc, variables_to_condition=(1,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint distribution equivalency (20 pts)\n",
    "Show by direct evaluation that $P(a,b,c) = P(a)P(c|a)P(b|c)$ for all values of $a$,$b$, and $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If my margilizaiton and conditional operations did as I intended and P(a,b,c) = P(a)P(c|a)P(b|c) The resulting array when I multiply (P_a * P_c_given_a * P_b_given_c) should be the same as the last column of the P_Table\n",
      "\n",
      "The original multidimensional P array: \n",
      "[[[0.192 0.144]\n",
      "  [0.048 0.216]]\n",
      "\n",
      " [[0.192 0.064]\n",
      "  [0.048 0.096]]]\n",
      "\n",
      "The array from multiplying (P_a * P_c_given_a * P_b_given_c):\n",
      "[[[0.192 0.216]\n",
      "  [0.048 0.324]]\n",
      "\n",
      " [[0.128 0.064]\n",
      "  [0.032 0.096]]]\n"
     ]
    }
   ],
   "source": [
    "print ('If my margilizaiton and conditional operations did as I intended and P(a,b,c) = P(a)P(c|a)P(b|c) \\\n",
    "The resulting array when I multiply (P_a * P_c_given_a * P_b_given_c) should be the same as the last column \\\n",
    "of the P_Table')\n",
    "print ()\n",
    "print ('The original multidimensional P array: ')\n",
    "print (P_abc)\n",
    "\n",
    "print ()\n",
    "print ('The array from multiplying (P_a * P_c_given_a * P_b_given_c):')\n",
    "print (P_a * P_c_given_a * P_b_given_c)\n",
    "\n",
    "#print ()\n",
    "#print (P_a, P_c_given_a, P_b_given_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Visualization (10 pts)\n",
    "\n",
    "Visualize the corresponding directed graph, either using graph software like [networkx](https://networkx.github.io/) or drawing it by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This graph shows the dependency of C on A and B on C\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.DiGraph()\n",
    "rvs = ['A','B','C']\n",
    "\n",
    "G.add_nodes_from(rvs)\n",
    "G.add_edges_from([('A', 'C'),('C','B')], weight=2)\n",
    "nx.draw_networkx(G, arrows=True)\n",
    "\n",
    "plt.show()\n",
    "print ('This graph shows the dependency of C on A and B on C')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Trick Question in Graphs (\\*) (10 pts)\n",
    "Plot the graph associated with\n",
    "$$\n",
    "P(A,B,C,D,E) = P(A)P(C)P(B|A,C)P(D|C)P(E|D),\n",
    "$$\n",
    "where $A,B,C,D,E \\in \\{0,1\\}$ with conditional probability tables\n",
    "\\begin{align}\n",
    "P(A=1) &= 0.3 \\nonumber \\\\\n",
    "P(C=1) &= 0.7 \\nonumber \\\\\n",
    "P(B=1|A,C) &= \\begin{cases} 0.3\\,\\mathrm{if}\\,A=0,C=0\\\\\n",
    "                          0.7\\,\\mathrm{if}\\,A=1,C=0\\\\\n",
    "                          0.1\\,\\mathrm{if}\\,A=0,C=1\\\\\n",
    "                          0.9\\,\\mathrm{if}\\,A=1,C=1 \\end{cases} \\nonumber \\\\\n",
    "P(D=1|C) &= \\begin{cases}   0.4\\,\\mathrm{if}\\,C=0\\\\\n",
    "                          0.2\\,\\mathrm{if}\\,C=1 \\end{cases} \\nonumber \\\\\n",
    "P(E=1|D) &= \\begin{cases}   0.1\\,\\mathrm{if}\\,D=0\\\\\n",
    "                          0.1\\,\\mathrm{if}\\,D=1 \\end{cases}. \\nonumber\n",
    "\\end{align}\n",
    "Compute the probability $P(A=1|E=1,C=1)$.  Be sure to think carefully about conditional independence before pulling out your calculator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dependecy graph shows A and C are both independent\n",
      "the above graph is illustraing  --->  P(A)P(C)P(B|A,C)P(D|C)P(E|D) this dependency\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.DiGraph()\n",
    "rvs = ['A','B','C','D','E']\n",
    "\n",
    "G.add_nodes_from(rvs)\n",
    "G.add_edges_from([('C','D'),('D','E'),('C','B'),('A','B')], weight=.1)\n",
    "nx.draw_networkx(G, arrows=True)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "print ('This dependecy graph shows A and C are both independent')\n",
    "print ('the above graph is illustraing  --->  P(A)P(C)P(B|A,C)P(D|C)P(E|D) this dependency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "The event A is not dependent on any other events  $ \\dot{.\\hspace{.015in}.}\\hspace{.1in}$   ...  \n",
    "$P(A=1|E=1,C=1)=P(A=1) = 0.3$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Gene Sequence Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Training a Markov model (40 pts)\n",
    "Load the file genes\\_training.p, which is available in this homework archive.  genes\\_training.p contains 2000 sequences, with each sequence $\\mathbf{s}$ consisting of 20 nucleobases $s_i \\in \\mathrm{Nu},\\; \\mathrm{Nu} = \\{A,T,G,C\\}$.  Each of these sequences is generated from one of two separate Markov processes.  The label (aka class) of the process that generated the sequence is given in the dataset. \n",
    "\n",
    "Learn the Markov model for each class given the training data.  **To do this, for each class compute the prior probability $\\mathbf{\\pi}_c$ of each nucleobase (the relative frequency of each nucleobase for each class, a vector of length 4) and the matrix of transition probabilities** \n",
    "$$\n",
    "\\mathcal{A}_{c,kj} = P(s_{i+1}=\\mathrm{Nu}_j|s_{i}=\\mathrm{Nu}_k),\n",
    "$$ \n",
    "which is a 4 by 4 matrix.  As a quick sanity check, each row of $\\mathcal{A}_c$ should sum to one.  **Using these priors and transition matrices, write a function that generates a new sequence given the class**, i.e. simulates a data point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns of class 0 transition matrix added to one.\n",
      "All columns of class 1 transition matrix added to one.\n",
      "[[0.14585319 0.04671115 0.34429616 0.4631395 ]\n",
      " [0.24204486 0.23213354 0.40114763 0.12467397]\n",
      " [0.67282249 0.18467475 0.06174201 0.08076075]\n",
      " [0.35344226 0.0474794  0.04510543 0.55397291]]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Define some useful constants\n",
    "N_nucleobases = 4\n",
    "N_classes = 2\n",
    "nucleobases = ['A','T','G','C']\n",
    "\n",
    "# Load the training data using pickle\n",
    "sequences,labels = pickle.load(open('genes_training.p','rb'))\n",
    "\n",
    "# Initialize the class priors and transition matrices\n",
    "pi_0 = np.zeros((N_nucleobases))\n",
    "pi_1 = np.zeros((N_nucleobases))\n",
    "\n",
    "A_0 = np.zeros((N_nucleobases,N_nucleobases))\n",
    "A_1 = np.zeros((N_nucleobases,N_nucleobases))\n",
    "# initialize count to help aid in debugging\n",
    "count0 =0\n",
    "count1 =0\n",
    "\n",
    "##### Train prior #####\n",
    "\n",
    "# Loop over all of the sequences and labels\n",
    "for s,l in zip(sequences,labels):\n",
    "    sequence_length = len(s)\n",
    "    #print (l)\n",
    "    for p in range(sequence_length):\n",
    "        if l == 0:\n",
    "            pi_0[0] += s[p].count('A')\n",
    "            pi_0[1] += s[p].count('T')\n",
    "            pi_0[2] += s[p].count('G')\n",
    "            pi_0[3] += s[p].count('C')\n",
    "            count0 +=1\n",
    "        if l == 1:\n",
    "            pi_1[0] += s[p].count('A')\n",
    "            pi_1[1] += s[p].count('T')\n",
    "            pi_1[2] += s[p].count('G')\n",
    "            pi_1[3] += s[p].count('C')\n",
    "            count1 += 1\n",
    "        #! s is a length 20 sequence of nucleobases.  For all s, count the number of times a given nucleobase \n",
    "        #!appears and record this information in the appropriate class prior (pi_0 or pi_1)\n",
    "        pass\n",
    "  \n",
    "\n",
    " # Convert from counts to probabilities by normalizing\n",
    "pi_0/=pi_0.sum()\n",
    "pi_1/=pi_1.sum()\n",
    "\n",
    "\n",
    "\n",
    "##### Train transition matrix #####\n",
    "for s,l in zip(sequences,labels):\n",
    "    #print (s,l)\n",
    "    sequence_length = len(s)\n",
    "    for p in range(sequence_length-1): # transitions from p to p+1 therefore must stop p one before it gets to end.\n",
    "        if l == 0:\n",
    "            if s[p] == 'A': \n",
    "                if s[p+1] == 'A':\n",
    "                    A_0[0,0] +=1\n",
    "                if s[p+1] == 'T':\n",
    "                    A_0[1,0] +=1\n",
    "                if s[p+1] == 'G':\n",
    "                    A_0[2,0] +=1\n",
    "                if s[p+1] == 'C':\n",
    "                    A_0[3,0] +=1\n",
    "            elif s[p] == 'T':\n",
    "                if s[p+1] == 'A':\n",
    "                    A_0[0,1] +=1\n",
    "                if s[p+1] == 'T':\n",
    "                    A_0[1,1] +=1\n",
    "                if s[p+1] == 'G':\n",
    "                    A_0[2,1] +=1\n",
    "                if s[p+1] == 'C':\n",
    "                    A_0[3,1] +=1\n",
    "            elif s[p] == 'G':\n",
    "                if s[p+1] == 'A':\n",
    "                    A_0[0,2] +=1\n",
    "                if s[p+1] == 'T':\n",
    "                    A_0[1,2] +=1 \n",
    "                if s[p+1] == 'G':\n",
    "                    A_0[2,2] +=1\n",
    "                if s[p+1] == 'C':\n",
    "                    A_0[3,2] +=1\n",
    "            elif s[p] == 'C':\n",
    "                if s[p+1] == 'A':\n",
    "                    A_0[0,3] +=1\n",
    "                if s[p+1] == 'T':\n",
    "                    A_0[1,3] +=1\n",
    "                if s[p+1] == 'G':\n",
    "                    A_0[2,3] +=1\n",
    "                if s[p+1] == 'C':\n",
    "                    A_0[3,3] +=1\n",
    "        if l == 1:\n",
    "            if s[p] == 'A': \n",
    "                if s[p+1] == 'A':\n",
    "                    A_1[0,0] +=1\n",
    "                if s[p+1] == 'T':\n",
    "                    A_1[1,0] +=1\n",
    "                if s[p+1] == 'G':\n",
    "                    A_1[2,0] +=1\n",
    "                if s[p+1] == 'C':\n",
    "                    A_1[3,0] +=1\n",
    "            elif s[p] == 'T':\n",
    "                if s[p+1] == 'A':\n",
    "                    A_1[0,1] +=1\n",
    "                if s[p+1] == 'T':\n",
    "                    A_1[1,1] +=1\n",
    "                if s[p+1] == 'G':\n",
    "                    A_1[2,1] +=1\n",
    "                if s[p+1] == 'C':\n",
    "                    A_1[3,1] +=1\n",
    "            elif s[p] == 'G':\n",
    "                if s[p+1] == 'A':\n",
    "                    A_1[0,2] +=1\n",
    "                if s[p+1] == 'T':\n",
    "                    A_1[1,2] +=1 \n",
    "                if s[p+1] == 'G':\n",
    "                    A_1[2,2] +=1\n",
    "                if s[p+1] == 'C':\n",
    "                    A_1[3,2] +=1\n",
    "            elif s[p] == 'C':\n",
    "                if s[p+1] == 'A':\n",
    "                    A_1[0,3] +=1\n",
    "                if s[p+1] == 'T':\n",
    "                    A_1[1,3] +=1\n",
    "                if s[p+1] == 'G':\n",
    "                    A_1[2,3] +=1\n",
    "                if s[p+1] == 'C':\n",
    "                    A_1[3,3] +=1    \n",
    "        #! s is a length 20 sequence of nucleoboases, for all s, count the number of times that a nucleobase \n",
    "        #! transitions to another nucleobase and record this information in the appropriate transition matrix (A_0 or A_1)\n",
    "        pass\n",
    "    \n",
    "# Convert from counts to probabilities by row normalization\n",
    "A_0/=A_0.sum(axis=0)[np.newaxis,:] # had to switch axis from 1 to 0 and np.newaxis to row because of how I\n",
    "A_1/=A_1.sum(axis=0)[np.newaxis,:] # defined my transition matrix above. A--> T not necessarily == T --> A\n",
    "\n",
    "#print (A_0)\n",
    "#print (A_1)\n",
    "#print(A_0.sum(axis=0))#[np.newaxis,:])\n",
    "#print(A_1.sum(axis=0)[:,np.newaxis])\n",
    "#print (1211+1704+2284+141)\n",
    "#print(1211+1023+2652+459)\n",
    "#print (1023+895+1325+1789)\n",
    "count = 0\n",
    "for i in range(4):   #check to see if each column adds up to one.\n",
    "    if (np.sum(A_0[:,i])) == 1:\n",
    "        count += 1  \n",
    "    if i == 3 and count == 4:\n",
    "        print ('All columns of class 0 transition matrix added to one.')\n",
    "        \n",
    "count = 0\n",
    "for i in range(4):   #check to see if each column adds up to one.\n",
    "    if (np.sum(A_1[:,i])) >.999999 and (np.sum(A_1[:,i])) < 1.000001:\n",
    "        count += 1 \n",
    "        \n",
    "    if i == 3 and count == 4:\n",
    "        print ('All columns of class 1 transition matrix added to one.')\n",
    "        \n",
    "print (A_1.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Notes\n",
    "    As a quick sanity check columns in my transition matrix (rows in how I think Doug formulated the problem & therefore my A.T should equal doug's A) should sum to one (adding values in the same column together, 'across all rows'). Similarly the prior probability arrays should also sum to one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prior probabilities P(A),P(T),P(G),P(C) used to generate the first nucleobase:\n",
      "priors in label 0:[0.28145 0.26535 0.31195 0.14125]\n",
      "priors in label 1:[0.332  0.1008 0.1901 0.3771]\n",
      "\n",
      "\n",
      "Transition Matrix for class 0: Columns and rows A, T, G, C, respectively\n",
      "[[0.22677903 0.20329889 0.44653982 0.17069543]\n",
      " [0.31910112 0.17786169 0.30527025 0.24581629]\n",
      " [0.42771536 0.26331479 0.24381209 0.31349944]\n",
      " [0.02640449 0.35552464 0.00437784 0.26998884]]\n",
      "\n",
      "Transition Matrix for class 1: A,T,G,C\n",
      "[[0.14585319 0.24204486 0.67282249 0.35344226]\n",
      " [0.04671115 0.23213354 0.18467475 0.0474794 ]\n",
      " [0.34429616 0.40114763 0.06174201 0.04510543]\n",
      " [0.4631395  0.12467397 0.08076075 0.55397291]]\n",
      "\n",
      "First order Markov Model Generated Sequences from prior and transition probabilites:\n",
      "label 0:\n",
      "['A', 'T', 'G', 'T', 'G', 'T', 'C', 'C', 'A', 'T', 'A', 'G', 'A', 'G', 'T', 'T', 'A', 'T', 'T', 'T']\n",
      "label 1:\n",
      "['T', 'G', 'A', 'C', 'A', 'C', 'C', 'A', 'A', 'T', 'A', 'G', 'A', 'G', 'A', 'G', 'A', 'C', 'C', 'C']\n"
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "##### Generate a synthetic sequence #####\n",
    "########################################\n",
    "\n",
    "\n",
    "def generate_new_sequence(A,pi,n=20):\n",
    "    \"\"\"  \n",
    "    Arguments:\n",
    "    A -> Nucleobase transition matrix\n",
    "    pi -> Prior\n",
    "    n -> length of sequence to generate\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(n):\n",
    "    # Draw from the prior for the first nucleobase\n",
    "        if i == 0:\n",
    "            s=[]\n",
    "            s.append(np.random.choice(nucleobases,p=pi))\n",
    "        else:    \n",
    "            if s[i-1] == 'A':\n",
    "                #print ('A')\n",
    "                s.append(np.random.choice(nucleobases,p=A[:,0]))\n",
    "            if s[i-1] == 'T':\n",
    "                #print ('T')\n",
    "                s.append(np.random.choice(nucleobases,p=A[:,1]))\n",
    "            if s[i-1] == 'G':\n",
    "                #print ('G')\n",
    "                s.append(np.random.choice(nucleobases,p=A[:,2]))\n",
    "            if s[i-1] == 'C':\n",
    "                #print ('C')\n",
    "                s.append(np.random.choice(nucleobases,p=A[:,3]))\n",
    "    #! Write the code that uses the transition matrix to produce a length n sample\n",
    "    return s\n",
    "\n",
    "seq_0=generate_new_sequence(A_0,pi_0)\n",
    "seq_1=generate_new_sequence(A_1,pi_1)\n",
    "\n",
    "\n",
    "\n",
    "print ('The prior probabilities P(A),P(T),P(G),P(C) used to generate the first nucleobase:')\n",
    "print ('priors in label 0:' + str(pi_0))\n",
    "print ('priors in label 1:' + str(pi_1))\n",
    "print ()\n",
    "print()\n",
    "print ('Transition Matrix for class 0: Columns and rows A, T, G, C, respectively')\n",
    "print (A_0)\n",
    "print ()\n",
    "print ('Transition Matrix for class 1: A,T,G,C')\n",
    "print (A_1)\n",
    "print()\n",
    "print ('First order Markov Model Generated Sequences from prior and transition probabilites:')\n",
    "print ('label 0:\\n' + str(seq_0))\n",
    "print ('label 1:\\n' +str(seq_1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. A Markov classifier (*) (20 pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the prior and transition probabilities gives you the ability to evaluate the likelihood of a sequence for a given class as:\n",
    "$$\n",
    "P(\\mathbf{s}) = P(s_1|\\mathbf{\\pi}_c) \\prod_{i=1}^{n-1} P(s_{i+1}|s_{i},\\mathcal{A}_c),\n",
    "$$\n",
    "where $\\mathbf{\\pi}_c$ is the class-conditioned prior probability, and $\\mathcal{A}_c$ is the class-conditioned transition matrix.  Comparing the computed likelihood for a given sequence between different classes forms the basis of a classifier in a very similar manner to naive Bayes.  The difference this time is that now each random variable depends on the one before it in the sequence, whereas in naive Bayes we assumed that all the random variables were independent.    \n",
    "\n",
    "Load the file genes\\_test.p, which is similar to genes\\_training.p.  **For each sequence, compute the likelihood for both classes and assign a label.  Compare this predicted label to the known one, and report the test set accuracy**.  As a point of comparison, my implementation achieved 98.7% accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percent accuracy of predicitng which class/label the test sequences came from is: \n",
      "98.0%\n"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "###################### MARKOV CLASSIFIER #######################\n",
    "################################################################\n",
    " \n",
    "# LOAD DATA\n",
    "sequences_test,labels_test = pickle.load(open('genes_test.p','rb'))\n",
    "\n",
    "\n",
    "def probability_of_sequence_conditioned_onlabel(A,pi):\n",
    "    \"\"\"function that takes a list of sequences and computes probability of each individual sequence (s) \n",
    "       by multiplying the prior|class by each transition probability, e.g. A-->G transition probability \n",
    "       times G-->T probability etc.\"\"\"\n",
    "     # initialize an empty array, making sure data typees are float64.\n",
    "    P_array = np.array([],dtype = np.float64)\n",
    "    count = 0\n",
    "    for s in sequences_test:\n",
    "        for i in range(len(s)):\n",
    "        \n",
    "            if i == 0:  # calc priors for first nucleotide in sequence\n",
    "                P = 1 \n",
    "                if s[0] == 'A':\n",
    "                    P *= pi[0]\n",
    "                elif s[0] == 'T':\n",
    "                    P *= pi[1]\n",
    "                elif s[0] == 'G':\n",
    "                    P *= pi[2]\n",
    "                elif s[0] == 'C':\n",
    "                    P *= pi[3]\n",
    "                #print(len(s))\n",
    "                #print (P)\n",
    "            \n",
    "            else:\n",
    "                #print (i)\n",
    "                if s[i-1]== 'A' and s[i] == 'A':\n",
    "                    P *= A[0,0]\n",
    "                    #print (P)\n",
    "                if s[i-1]== 'A' and s[i] == 'T':\n",
    "                    P *= A[1,0]\n",
    "                if s[i-1]== 'A' and s[i] == 'G':\n",
    "                    P *= A[2,0]\n",
    "                if s[i-1]== 'A' and s[i] == 'C':\n",
    "                    P *= A[3,0]\n",
    "                if s[i-1]== 'T':\n",
    "                    if s[i]== 'A':\n",
    "                        P *= A[0,1]\n",
    "                    if s[i]== 'T':\n",
    "                        P *= A[1,1]\n",
    "                    if s[i]== 'G':\n",
    "                        P *= A[2,1]\n",
    "                    if s[i]== 'C':\n",
    "                        P *= A[3,1]   \n",
    "                if s[i-1]== 'G':\n",
    "                    if s[i]== 'A':\n",
    "                        P *= A[0,2]\n",
    "                    if s[i]== 'T':\n",
    "                        P *= A[1,2]\n",
    "                    if s[i]== 'G':\n",
    "                        P *= A[2,2]\n",
    "                    if s[i]== 'C':\n",
    "                        P *= A[3,2]\n",
    "                if s[i-1]== 'C':\n",
    "                    if s[i]== 'A':\n",
    "                        P *= A[0,3]\n",
    "                    if s[i]== 'T':\n",
    "                        P *= A[1,3]\n",
    "                    if s[i]== 'G':\n",
    "                        P *= A[2,3]\n",
    "                    if s[i]== 'C':\n",
    "                        P *= A[3,3] \n",
    "                if i == len(s)-1:\n",
    "                    \n",
    "                    P_array=np.append(P_array,P) # append the P scalar for specific sequence onto an array of such values corresponding\n",
    "                    count +=1         # to P of specific sequences with indices corresponding to indicies of s's in seuqence_test\n",
    "                    \n",
    "                                         \n",
    "    return P_array\n",
    "\n",
    "#print (P_0.dtype())\n",
    "#print (len(sequences_test))\n",
    "#print (count)\n",
    "#print (P)\n",
    "Sequence_Probability_0 = probability_of_sequence_conditioned_onlabel(A_0,pi_0)\n",
    "#print (Sequence_Probability_0)\n",
    "\n",
    "Sequence_Probability_1 = probability_of_sequence_conditioned_onlabel(A_1,pi_1)\n",
    "#print (Sequence_Probability_1)\n",
    "\n",
    "Ratio_Pgiven0_to_Pgiven1 = Sequence_Probability_0/Sequence_Probability_1\n",
    "#print (Ratio_Pgiven0_to_Pgiven1)\n",
    "\n",
    "# Caclulating Accuracy by dividing the P given 0 by the P given 1 for each sequence. \n",
    "# if larger than 1 that means there was a larger P of label 0 \n",
    "predicted_label=[]\n",
    "for i in range(len(Ratio_Pgiven0_to_Pgiven1)):\n",
    "    if Ratio_Pgiven0_to_Pgiven1[i] >= 10:\n",
    "        predicted_label.append(0)\n",
    "    else:\n",
    "        predicted_label.append(1)\n",
    "\n",
    "count = 0 \n",
    "for i in range(len(predicted_label)):\n",
    "    if predicted_label[i] == labels_test[i]:\n",
    "        count += 1\n",
    "    \n",
    "\n",
    "Accuracy = float(count)/(float(len(predicted_label)))        \n",
    "\n",
    "print ('The percent accuracy of predicitng which class/label the test sequences came from is: \\n' + str(100*Accuracy) + '%')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markov Model accuracy: \n",
      " 0.986\n"
     ]
    }
   ],
   "source": [
    "####### Diff method of predicting label ########\n",
    "\n",
    "Sequence_Probability_0 = probability_of_sequence_conditioned_onlabel(A_0,pi_0)\n",
    "#print (Sequence_Probability_0)\n",
    "Sequence_Probability_1 = probability_of_sequence_conditioned_onlabel(A_1,pi_1)\n",
    "#print (Sequence_Probability_1)\n",
    "\n",
    "predicted_labels = []\n",
    "for i in range(len(Sequence_Probability_0)):\n",
    "    if Sequence_Probability_0[i]>Sequence_Probability_1[i]:\n",
    "        predicted_labels.append(0)\n",
    "    else:\n",
    "        predicted_labels.append(1)\n",
    "\n",
    "count = 0\n",
    "for i in range(len(labels_test)):\n",
    "    if int(labels_test[i]) == int(predicted_labels[i]):\n",
    "        count += 1\n",
    "\n",
    "mm_accuracy = count/len(labels_test)        \n",
    "print ( 'Markov Model accuracy: \\n', mm_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent accuracy: 98.6\n",
      "Using the log ratio gave me the same accuracy. As it should, since A.) the probabilities arent small enoughto cause truncation and b.) because I didnt take the log throughout the calculations but only at the end.\n"
     ]
    }
   ],
   "source": [
    "## Calculating accuracy via the log ratio method:\n",
    "\n",
    "log_ratio_P = np.log(Sequence_Probability_0/Sequence_Probability_1)\n",
    "    \n",
    "\n",
    "predicted_label =[]\n",
    "for i in range(len(Sequence_Probability_0)):\n",
    "    if log_ratio_P[i] > 0:\n",
    "        predicted_label.append(0)\n",
    "    else:\n",
    "        predicted_label.append(1)\n",
    "count = 0\n",
    "for i in range(len(predicted_label)):\n",
    "    if predicted_label[i]==labels_test[i]:\n",
    "        count +=1\n",
    "    #else:\n",
    "        #print (i)  #figuring out where I mispredicted. \n",
    "        \n",
    "\n",
    "print ('Percent accuracy: ' + str(100*count/len(predicted_label)))\n",
    "print ('Using the log ratio gave me the same accuracy. As it should, since A.) the probabilities arent small enough\\\n",
    "to cause truncation and b.) because I didnt take the log throughout the calculations but only at the end.')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Notes\n",
    "I achieved a high accuracy. However, a uniform prior would get a near a 50% classification accuracy since there are only two labels. The key ingredient here is the transition probailities. And if a transition is 2 times as likely in class 0 vs class 1 and the next transition is 2 times as likely in class 0 vs class 1, after multiplying them we have class 0 being 4 times as likely. In this way small differences in transition probabilities can multiply up and become relatively large differnces after a sufficient number of transitions has been analyzed. Of course, this is dependent on there being discernible differences between the transition probabilities of the differing classes and having enough data to detect these differences, both with training data and test data. \n",
    "\n",
    "Next i compare this markov model to a simple naive bayes to put my classification accuracy in context. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy: 89.9%\n",
      "Markov Model accuracy:  98.6 %\n"
     ]
    }
   ],
   "source": [
    "#####################################################\n",
    "######## Naive Bayes for Model Comparison ###########\n",
    "#####################################################\n",
    "\n",
    "# LOAD DATA#\n",
    "sequences_test,labels_test = pickle.load(open('genes_test.p','rb'))\n",
    "\n",
    "\n",
    "def Naive_Bayes(pi):\n",
    "    \"\"\"function that takes a list of sequences and computes probability of each individual sequence (s) \n",
    "       by multiplying the prior|class by the prior|class of the next element in sequence. Made in order to test how much\n",
    "       better my markov model is compared to a naive bayes approach.\"\"\"\n",
    "     # initialize an empty array, making sure data typees are float64.\n",
    "    P_array = np.array([],dtype = np.float64)\n",
    "    count = 0\n",
    "    for s in sequences_test:\n",
    "        P = 1\n",
    "        for i in range(len(s)):\n",
    "                if s[i] == 'A':\n",
    "                    P *= pi[0]\n",
    "                elif s[i] == 'T':\n",
    "                    P *= pi[1]\n",
    "                elif s[i] == 'G':\n",
    "                    P *= pi[2]\n",
    "                elif s[i] == 'C':\n",
    "                    P *= pi[3]\n",
    "                if i == len(s)-1:\n",
    "                    P_array = np.append(P_array,P)\n",
    "    return P_array\n",
    "\n",
    "NB_P_0 = Naive_Bayes(pi_0)\n",
    "NB_P_1 = Naive_Bayes(pi_1)\n",
    "Ratio_NB_P_0_P_1 = NB_P_0/NB_P_1\n",
    "Naive_Labels = np.array([])\n",
    "\n",
    "for i in range(len(Ratio_NB_P_0_P_1)):\n",
    "    if Ratio_NB_P_0_P_1[i]>1:\n",
    "        Naive_Labels = np.append(Naive_Labels, 0)\n",
    "    else:\n",
    "        Naive_Labels = np.append(Naive_Labels, 1)\n",
    "count = 0    \n",
    "for i in range(len(Naive_Labels)):\n",
    "    if Naive_Labels[i] != labels_test[i]:\n",
    "        count +=1\n",
    "print ('Naive Bayes accuracy: ' + str((len(Naive_Labels)-count)/len(Naive_Labels)*100) + '%')\n",
    "print ('Markov Model accuracy: ', mm_accuracy * 100,'%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Naive Bayes is much higher than an uniformed prior of ~50% accuracy and the Markov Model is yet again much better than the Naive Bayes. However, one wonders if my markov model is overtrained when evaluated against other potential data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
